export const enTranslations = {
    tagline: "We Design Intelligence Through Architecture.",
    intro: {
        title: "About EXITON Lab",
        content: `
      <p>EXITON Lab is a small, experimental AI research unit with a mission to <strong>"sit at the same table as AI and jointly design intelligent architectures."</strong></p>
      <ul>
        <li>Focus on <strong>Small / Local Language Models (SLMs)</strong> rather than massive cloud models.</li>
        <li>Prioritize <strong>"How to Structure (Architecture)"</strong> over "Learning a lot".</li>
        <li>Full use of <strong>AI as a co-researcher</strong> in every aspect of the research process.</li>
        <li>Yet, the final responsibility and judgment lies with the <strong>human researcher</strong>.</li>
      </ul>
      <blockquote>
        <p><strong>Structure > Scale</strong></p>
      </blockquote>
    `
    },
    sections: {
        about: {
            title: "What is EXITON Lab?",
            content: `
      <p>EXITON Lab is a small, experimental AI research unit with a mission to <strong>"sit at the same table as AI and jointly design intelligent architectures."</strong></p>
      <ul>
        <li>Focus on <strong>Small / Local Language Models (SLMs)</strong> rather than massive cloud models.</li>
        <li>Prioritize <strong>"How to Structure (Architecture)"</strong> over "Learning a lot".</li>
        <li>Full use of <strong>AI as a co-researcher</strong> in every aspect of the research process.</li>
        <li>Yet, the final responsibility and judgment lies with the <strong>human researcher</strong>.</li>
      </ul>
      <blockquote>
        <p><strong>Structure > Scale</strong></p>
      </blockquote>
    `
        },
        research: {
            title: "Research Focus",
            intro: "EXITON Lab's research themes can be organized into the following four axes.",
            subsections: [
                {
                    title: " Reliability & Validation of AI Systems",
                    content: "<p>Instead of hoping LLMs are 'accidentally' correct, how can we make them <strong>structurally unbreakable</strong>? We research logic validation layers (Validator / Critic / Rule Engine) to detect and suppress hallucination.</p>"
                },
                {
                    title: "Cognitive Operating Systems",
                    content: "<p>Exploring architectures that treat LLMs not as a 'one-shot answer machine' but as a <strong>component like a CPU within an OS</strong>. Keywords: ContextOS, Semantic CPU, State Manager.</p>"
                },
                {
                    title: "Architecture-Driven Intelligence",
                    content: "<p>We believe <strong>'Intelligence stability is determined by architecture, not parameter count.'</strong> We explore how to decompose modules to solve large problems with small models.</p>"
                },
                {
                    title: "Human–AI Co-Research",
                    content: "<p>Redesigning the research process itself as <strong>'Human × AI Collaboration'</strong>. From brainstorming to coding and paper writing, we work with AI as a partner, while retaining human responsibility.</p>"
                }
            ]
        },
        principles: {
            title: "Research Principles",
            content: "<p><strong>Transparency</strong>: We record which AI models are used.<br><strong>Human Responsibility</strong>: Humans bear final responsibility for truth and ethics.<br><strong>Reproducibility</strong>: We aim for open, reproducible benchmarks.<br><strong>Privacy</strong>: No confidential data sent to public LLMs.<br><strong>Ethical Partnership</strong>: AI is a partner, not a magic box.</p>"
        },
        projects: {
            title: "Current Projects",
            content: "<p><strong>Logic Validator</strong>: A project to add a 'logic inspection line' to LLM outputs. Preparation for ICLR 2026.<br><br><strong>ContextOS</strong>: Reinterpreting LLMs as an OS-layer for intelligence. State management and tool execution orchestration.</p>"
        },
        people: {
            title: "People",
            content: "<p><strong>Kousuke Nakamura</strong> (Founder / Lead Researcher)<br>Background: Electrical & Electronics Engineering.<br>Interests: AI Architecture, SLM Agents, Reliability.</p>"
        },
        notes: {
            title: "Blog & Notes",
            content: "<p>Coming Soon. Will feature reading notes, architecture memos, and SLM experiment logs.</p>"
        },
        contact: {
            title: "Contact",
            content: "<p>For collaboration, technical consultation, or mentoring, please contact us via the official channels (to be added).</p>"
        }
    }
};
